{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "x6ZHOucvOOza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "nltk.download(\"movie_reviews\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK8D8UDUCZlc",
        "outputId": "402e646f-184d-4e35-8b18-a58c1eecd98b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "import random\n",
        "\n",
        "# X: words / Y: polarity\n",
        "train_X, train_Y = [], []\n",
        "test_X, test_Y = [], []\n",
        "\n",
        "random.seed(0)\n",
        "for polarity in movie_reviews.categories():\n",
        "    for fid in movie_reviews.fileids(polarity):\n",
        "        # 1/5 for test; 4/5 for train\n",
        "        if random.randrange(5) == 0:\n",
        "            test_X.append([w for w in movie_reviews.words(fid)])\n",
        "            test_Y.append(polarity)\n",
        "        else:\n",
        "            train_X.append([w for w in movie_reviews.words(fid)])\n",
        "            train_Y.append(polarity)\n",
        "\n",
        "print(train_X[0], train_Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyfLD0SRChBr",
        "outputId": "575774bc-5db9-4f9d-a4c0-16896e587cf1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')'] neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Construction\n",
        "\n",
        "$\\bar{y} = \\text{arg}\\max_{y \\in \\mathbf{y}} P(y|x) = \\text{arg}\\max_{y \\in \\mathbf{y}} P(y) \\prod_{i=1}^n \\frac{P(x_i|y)}{P(x_i)} = \\text{arg}\\max_{y \\in \\mathbf{y}} P(y) \\prod_{i=1}^n P(x_i|y)$\n",
        "\n",
        "$P(x_i|y)=\\frac{C(x_i, y) + k}{C(y) + |\\mathbf{y}| \\times k}$\n",
        "\n",
        "$\\bar{y} = \\textrm{arg} \\max_{y \\in \\mathbf{y}} \\log P(y) + \\sum_{i=1}^n \\log \\frac{C(x_i, y) + k}{C(y) + k|\\mathbf{y}|}$\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "8TiSvHHAhy6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "import numpy as np\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, k=0.1):\n",
        "        self.k = k\n",
        "        self.features = set()\n",
        "        self.class_feature_counts = defaultdict(Counter)\n",
        "        self.class_counts = Counter()\n",
        "        self.total = 0\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.vectorizer = CountVectorizer(max_df=0.65, min_df=10)\n",
        "        self.selector = SelectKBest(mutual_info_classif, k=2000)\n",
        "\n",
        "    def train(self, train_X, train_Y):\n",
        "        word_counts = self.vectorizer.fit_transform([' '.join(tokens) for tokens in train_X])\n",
        "        self.selector.fit(word_counts, train_Y)\n",
        "        feature_names = np.array(self.vectorizer.get_feature_names_out())[self.selector.get_support()]\n",
        "\n",
        "        for tokens, label in zip(train_X, train_Y):\n",
        "            self.class_counts[label] += 1\n",
        "            self.total += 1\n",
        "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
        "            for token in set(tokens):\n",
        "                if token in feature_names:\n",
        "                    self.features.add(token)\n",
        "                    self.class_feature_counts[label][token] += 1\n",
        "\n",
        "    def probabilities(self, token):\n",
        "        probs = {}\n",
        "        for cls, cls_cnt in self.class_counts.items():\n",
        "            probs[cls] = (self.class_feature_counts[cls][token] + self.k) / (cls_cnt + len(self.class_counts) * self.k)\n",
        "        return probs\n",
        "\n",
        "    def predict(self, tokens):\n",
        "        tokens = set([self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words])\n",
        "        log_probs = Counter()\n",
        "        for cls, cls_cnt in self.class_counts.items():\n",
        "            log_probs[cls] = math.log(cls_cnt / self.total)\n",
        "        for token in self.features:\n",
        "            probs = self.probabilities(token)\n",
        "            if token in tokens:\n",
        "                for cls, prob in probs.items():\n",
        "                    log_probs[cls] += math.log(prob)\n",
        "            else:\n",
        "                for cls, prob in probs.items():\n",
        "                    log_probs[cls] += math.log(1.0 - prob)\n",
        "        # Return the argmax of log_probs and all log_probs\n",
        "        return max(log_probs, key=log_probs.get), log_probs"
      ],
      "metadata": {
        "id": "fobAkWURDr6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc78b466-b355-46c1-bce2-3cc312df9869"
      },
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Model"
      ],
      "metadata": {
        "id": "GE_Ht04wh3Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NaiveBayesClassifier()\n",
        "model.train(train_X, train_Y)"
      ],
      "metadata": {
        "id": "5rdZQ-aJhtld"
      },
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Taken from https://www.imdb.com/review/rw0990793/?ref_=tt_urv\n",
        "review = \"\"\"A whimsical, often spectacular view of a future in which advances in technology dominate the world. It is well shot and although slow-moving it is intense and enjoyable throughout. The featuring of classical music to establish atmosphere works brilliantly; it provides a feeling of awe, mystery and intrigue Â– the same aura that Walt Disney worked in creating 'Fantasia'. The special effects, both sound and visual, are still spellbinding by the standards of today's technology. Aside from the technical pluses of the film, it stands strong as it is one of not many films out there that has something important to say about humankind, and where the human race is heading in terms of our increasing reliance on machines and our unquenchable thirst to discover. Despite an ending that is hard to understand, it is even harder to overlook this film a true cinema classic.\"\"\"\n",
        "\n",
        "model.predict(word_tokenize(review))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeH93xRFZ1EF",
        "outputId": "c06bbb31-9fe0-4da9-bd5c-08bdd19e2d6d"
      },
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pos', Counter({'neg': -195.86971202641826, 'pos': -192.72106891678405}))"
            ]
          },
          "metadata": {},
          "execution_count": 352
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct, total = 0, 0\n",
        "\n",
        "for x, y in zip(test_X, test_Y):\n",
        "    prediction, _ = model.predict(x)\n",
        "    if prediction == y:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "print(\"%d / %d = %g\" % (correct, total, correct / total))"
      ],
      "metadata": {
        "id": "U1lkFRJ0FKbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b92bfd94-3b0a-497e-cf3b-9f30be5a098f"
      },
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "362 / 422 = 0.85782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring important features"
      ],
      "metadata": {
        "id": "pmqZUqeTgjzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prob_class_given_feature(feature, cls, model):\n",
        "    probs = model.probabilities(feature)\n",
        "    return probs[cls] / sum(probs.values())\n",
        "\n",
        "print(sorted(model.features, key=lambda t: prob_class_given_feature(t, \"pos\", model), reverse=True)[:30])\n",
        "print(sorted(model.features, key=lambda t: prob_class_given_feature(t, \"neg\", model), reverse=True)[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xofcbwlXgluq",
        "outputId": "5ab2cf23-7409-4f1b-8eba-4df539de418f"
      },
      "execution_count": 354,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thematic', 'dread', 'astounding', 'reminder', 'kenobi', 'turturro', 'naval', 'en', 'seamless', 'fascination', 'denial', 'lovingly', 'keen', 'masterfully', 'vocal', 'maintains', 'strongest', 'effortlessly', 'fable', 'concentration', 'avoids', 'guardian', 'darker', 'affecting', 'accessible', 'outstanding', 'hatred', 'ideology', 'annual', 'rousing']\n",
            "['hudson', 'sans', '3000', 'illogical', 'overwrought', 'don', 'plant', 'schumacher', 'uh', 'ugh', 'roberts', 'henstridge', 'insulting', 'welles', 'excruciatingly', 'unimaginative', 'fairness', 'ludicrous', 'terri', 'furniture', 'predator', 'beard', 'silverstone', 'wasting', 'coyote', 'justin', 'turkey', 'insipid', 'suvari', 'tripe']\n"
          ]
        }
      ]
    }
  ]
}